{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "V100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD1KfLPNXYmw"
   },
   "source": "# **Class 05**: Deep Learning with Neural Networks"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Large Language Models\n",
    "\n",
    "> Large Language Models (LLMs) are at the intersection of NLP (Natural Language Processing) and Deep Learning.   \n",
    "They can generate human-like text responses.\n",
    "\n",
    "* **LARGE**: Training data, required computing power, number of model parameters\n",
    "* **LANGUAGE**: Human-like text\n",
    "* **MODEL**: Simplification of some complex phenomenon\n",
    "\n",
    "![Source: DataCamp](https://mapXP.app/MBA742/AI_ML_DL_NLP_LLM.png \"Large Language Models\")\n"
   ],
   "metadata": {
    "id": "NMwMdYqd3sKY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. The Road to Deep Learning\n",
    "\n",
    "## 1.1 Models\n",
    "\n",
    "> A model is a simplification of some complex phenomenon.\n",
    "\n",
    "![Car vs Car Model](https://mapXP.app/MBA742/model.png \"Model\")\n",
    "\n",
    "### ***A model of a car is a smaller, simpler version of that car.***"
   ],
   "metadata": {
    "id": "jPEi2Vjn3sPz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Machine Learning\n",
    "\n",
    "*  Machine learning is the study of algorithms that allow computer programs to automatically improve through experience.\n",
    "* ML creates behavior by taking in data, forming a model, and then executing the model\n",
    "* Hard to model language (or many other phenomena) with a bunch of rules (e.g., if-else statements)\n",
    ">We use algorithms that can find patterns in data to model a phenomenon\n",
    "\n",
    "### ***If we can create a model of a car, then we might be able to create a model of language***\n",
    "> We use machine learning to create smaller, simpler versions of human language"
   ],
   "metadata": {
    "id": "PG9M2ZI3D0li"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.3 Neural Networks\n",
    "\n",
    "> **Neural Networks are one way to learn a model from data**\n",
    "\n",
    "* Idea is roughly based on how the human brain is made\n",
    "  * Network of interconnected brain cells called ***neurons***\n",
    "  * Neurons ***pass electrical signals*** back and forth\n",
    "  * Somehow ***allowing us to do*** all the ***things*** we do.\n",
    "* Frank Rosenblatt invented Artificial Neutal Networks that comprise multiple layers of neurons to \"learn\" patterns in the 1950s.\n",
    "    * Decades later, we figured out how to train them.\n",
    "* Neural networks are very inefficient.\n",
    "    * Decades later, we had powerful enought hardware to train them at scale\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "oce0yNM5GNMK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.1 Self-Driving Car with Neural Networks\n",
    "\n",
    "#### **We will use a toy example to explore how neural networks work: no math involved!**\n",
    "\n",
    "* You want to build self-driving car and need:\n",
    "  * **Sensors**: proximity sensors on front, back, and side [ report 1.0 if something is close, else 0.0]\n",
    "  * **Servos**: little motors that can push the accelerator and brakes [ 1.0 max push, 0.0 no push],   \n",
    "  and turn the steering wheel [-1.0 full left, 1.0 full right]\n",
    "  * **Electrical wiring**: connect all components.\n",
    "  * **Driving Data**: Recordings how how people drive\n",
    "    * Accelerate when clear\n",
    "    * Break when there is obstruction\n",
    "    * Steer left or right to change lanes\n",
    "    * Combinations of all these (in sequence and/or simultaneously)\n"
   ],
   "metadata": {
    "id": "Hhelj90TGNRs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.2 Wiring-up the Neural Network\n",
    "\n",
    "#### **Install your Self-Driving System**\n",
    "* Need to connect sensors to servos in your car.\n",
    "* Unclear how to wire sensors and servos exactly.\n",
    "> Connect ***every sensor with every servo***\n",
    "\n",
    "![NN for self driving car](https://mapXP.app/MBA742/MRiedl_self-drive1.jpg \"Mark Riedl Self Driving Car Example\")\n",
    "\n",
    "#### **Take car for a drive**\n",
    "* Same electricity flows from all sensors to all servos\n",
    "* Brake, accelerate, and steer equally all at once\n",
    "> CRASH!\n",
    "\n",
    "\n",
    "![NN for self driving car](https://mapXP.app/MBA742/MRiedl_self-drive2.gif \"Mark Riedl Self Driving Car Example\")\n"
   ],
   "metadata": {
    "id": "P0xYnwfhGNUU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **How do we fix it?**\n",
    "\n",
    "> **Fix 1**: Need electrical signal to ***flow more freely between certain sensors and certain servos*** than others.  \n",
    "  * We want electrical signal to flow more freely from the front proximity sensors to the brakes and not to the steering wheel.\n",
    "\n",
    "> **Fix 2**: Need to ***control signal strength*** to each servo ***conditional on the signal strength of the sensor***.\n",
    "  * We want to send a stronger signal to the accelerator when the signal from the proximity sensor is low.  \n",
    "\n",
    "> **Fix 3**: May need to ***combine the signals from multiple sensors*** and ***process these some more*** to better address the right servos.  \n",
    "  * We want the steering servo to turn left and the brake to engage when the front and right proximity sensors both send a signal.  \n",
    "\n",
    "#### ***That's a lot of moving parts to worry about (no pun inteded) !!!***\n"
   ],
   "metadata": {
    "id": "AKtaSY7lGNXS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Neural Networks to the Rescue!**\n",
    "Add layers between the sensors and servos...\n",
    "\n",
    "#### **Set-up**\n",
    "* Create an Input Layer of ***4 NEURONS***: Each sensor gets hooked up to it's own *Neuron*\n",
    "* Create an Output Layer of ***3 NEURONS***: Each servo gets hooked up to it's own *Neuron*\n",
    "* Connect ***all NEURONS*** (with \"wires\")\n",
    "\n",
    "#### **Implement Fix 1**\n",
    "***Allow for different signal strengths between *Neurons****\n",
    "  > Introduce ***WEIGHTS*** between connected *Neurons*:   \n",
    "  * more weight = stronger signal passed forward\n",
    "  * less weight = weaker signal passed forward\n",
    "\n",
    "#### **Implement Fix 2**\n",
    "***Modify signal strength passed to next Neuron based on the received signal***\n",
    "  > Add ***GATES*** \"between\" *Neurons*  \n",
    "  * The \"rules\" of how a gate operates are called its ***ACTIVATION FUNCTION***\n",
    "  * An ***ACTIVATION FUNCTION*** is a mathematical function applied to the output signal of a *Neuron* before it is passed forward to the next *Neuron(s)*.   \n",
    "      * *Example*: the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) (which squashes outputs to a range between 0 and 1).\n",
    "      * Note: An *activation function* can reduce the signal to zero, effectively \"switching off\" the *Neuron* in the network\n",
    "  * We can adjust these \"rules\" (*activation functions*) with what we call ***BIASES*** in Neural Networks.\n",
    "\n",
    "#### **Implement Fix 3**\n",
    "***Combine signals and process further for more nuanced driving***\n",
    "  > Insert ***HIDDEN LAYERS*** of additional *NEURONS* between Input and Output Layer:   \n",
    "  * Capture Non-Linearity\n",
    "  * Enable Feature Abstraction and Hierarchical Learning\n",
    "  * Increase Flexibility and Functionality (more nuanced patterns)\n"
   ],
   "metadata": {
    "id": "1NcvfqLhPRiU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![NN for self driving car](https://mapXP.app/MBA742/self-drive3.jpg \"Adapted from Mark Riedl Self Driving Car Example\")"
   ],
   "metadata": {
    "id": "bwxGyLIycd-m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3.3 Learning to Self-Drive\n",
    "\n",
    "**So far we**:\n",
    "1. Set-up our Neural Network based on inputs (sensors) and outputs (servos)\n",
    "2. Decided on the number of hidden layers (here, 1) and number of *Neurons* in each hidden layer (here, 5).\n",
    "3. Defined *Activation Functions*\n",
    "   * Typically same for all hidden layers.\n",
    "   * For output layer often different to match objective of the Neural Network.\n",
    "\n",
    "These are commonly called the model's ***HYPERPARAMETERS***: these are decisions made by the model creator that affect the model's internal performance but that are not evident from the output.   \n",
    "\n",
    "**Signals from sensors are now passsed forward through our Neural Network**\n",
    "* We call this ***FEEDFORWARD PROPAGATION***\n",
    "* It's a one-way street\n",
    "\n",
    "**We can change how signals are passed forward by adjusting**:\n",
    "* Weights (of Fix 1)\n",
    "* Biases (of Fix 2).  \n",
    "\n",
    "These are typically referred to as the model's **PARAMETERS** that need to learned.\n",
    "\n",
    "**We learn weights and biases**\n",
    "* From our collected driving data\n",
    "* By adjusting them such that the servos are operated (correctly) based on the sensor inputs\n",
    "* Using what is called ***BACKPROPAGATION***\n",
    ">  Backpropagation is a training algorithm used to optimize the weights and biases by minimizing the difference between the network's predicted output and the actual target values.\n",
    "(Parameters are adjusted by the model; hyperparameters by the designer.)\n",
    "\n"
   ],
   "metadata": {
    "id": "bISGW24qxnka"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![NN for self driving car](https://mapXP.app/MBA742/self-drive4a.gif \"Adapted from Mark Riedl Self Driving Car Example\")"
   ],
   "metadata": {
    "id": "DrtPIlObroBV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Deep Learning\n",
    "\n",
    "Deep Learning is an **advanced subset of machine learning** that involves\n",
    "* the use of *Neural Networks*\n",
    "* with ***multiple*** *Hidden Layers*\n",
    "\n",
    "> For our self-driving car example, deep learning is analogous to ***significantly increasing the complexity and depth*** of the circuitry.\n",
    "\n",
    "**Deep Neural Networks**\n",
    "* apply more varied non-linear transformations (activation functions)\n",
    "* convolutional filters (in CNNs)\n",
    "* recurrent connections (in RNNs)\n",
    "\n",
    "and more, to process and learn from data.   \n",
    "\n",
    "\n",
    "**Deep learning models typically learn from data by**\n",
    "* adjusting the network's parameters (weights and biases),\n",
    "* incrementally using backpropagation and gradient descent (or its variants),\n",
    "* similar to shallow neural networks,\n",
    "* but with added complexity of adjustments across many more layers.\n"
   ],
   "metadata": {
    "id": "Y7vzSDdDxhuw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Language Models\n",
    "\n",
    "We use a ***Neural Network*** to manipulate the acceleration, braking, and steering of a self-driving car ***the same way humans*** did in the recorded driving data.\n",
    "\n",
    "> **Let's treat language the same way!**\n",
    "\n",
    "**Task:** Build a Neural Network\n",
    "  * based on text written by humans,\n",
    "  * that outputs a sequence of words,\n",
    "  * which looks like word sequences produced by humans.\n",
    "\n",
    "***From Car to Language:***\n",
    "* Sensors (Input) ---> Words (tokens)\n",
    "* Servos (Output) ---> Words (tokens)\n",
    "\n",
    "> ***Given a bunch of input words (tokens), predict the right output word (token)***\n",
    "\n",
    "> ### \"Once upon a ____ \"  \n",
    ">* *time* or\n",
    ">* *goat* [?](https://youtu.be/Ud_lhpOqZzY?si=s_Fm_mZWa68KGvWu)\n",
    "\n",
    ">***Which word (token) is more likely?***\n",
    ">\n",
    ">#### $P(\\text{time} \\,|\\, \\text{once}, \\text{upon}, \\text{a})$.\n",
    ">\n",
    ">*which generalizes to*\n",
    ">\n",
    ">#### $P(\\text{word}_n \\,|\\, \\text{word}_1, \\text{word}_2, \\ldots, \\text{word}_{n-1})$"
   ],
   "metadata": {
    "id": "ERI980r7KLJO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Building a Language Neural Network\n",
    "\n",
    "\n",
    "> ### Imagine an old-fashioned typewriter with keys and striker arms:\n",
    "![Words Typewriter](https://mapXP.app/MBA742/tyepwriter_words.jpg \"Words Typewriter\")\n",
    "\n",
    "#### **HOWEVER**: Instead of a **Key** and **Striker** for ***each Letter***, we have them for ***each Word*** (w)\n",
    "\n",
    "- ***WORDS*** (or parts of them) can also be referred to as ***TOKENS***\n",
    "- Let's assume the English language had 50,000 words (or tokens): the vocabulary\n",
    "> That's a HUGE Typewriter!\n",
    "\n",
    "#### Neural Network Set-up\n",
    "* Typewriter **Keys** ==> **Input Layer** with 50,000 Neurons; one for each word (token)\n",
    "* Typewriter **Strikers** ==> **Output Layer** with 50,000 Neurons; one for each word (token)\n",
    "* All Keys to all Strikers ==> **Wiring**\n",
    "\n",
    "#### Task\n",
    "* Pick the correct Striker given the words punched into the Keys.\n",
    "\n",
    "> For a typed phrase that triggers Neurons of the Input Layer, send strongest signal to the Neuron on the Output Layer that corresponds to the word to complete the phrase.\n",
    "\n",
    "#### Challenge\n",
    "* Even a simple language model that takes in a single word to predict a single word requires:\n",
    "  * 50,000 Neurons on the Input Layer\n",
    "  * 50,000 Neurons on the Output Layer\n",
    "  * 50,000 x 50,000 = **2.5 billion wires between layers**\n",
    "\n",
    "![NN Language Typewriter](https://mapXP.app/MBA742/MRiedl_LangTyp1.gif \"credit: Mark Riedl\")\n",
    "\n",
    "#### **That is HUGE!**\n",
    "*More bad news...*\n",
    "\n",
    "* To fill the missing word (blank) in \"Once upon a ___\" we need to consider all three words in the input:\n",
    "  * 50,000 x 3 = 150,000 Neurons on Input Layer\n",
    "  * 50,000 Neurons on Output Layer\n",
    "  * 150,000 x 50,000 = **7.5 billion wires between layers**\n",
    "![NN Language Typewriter](https://mapXP.app/MBA742/MRiedl_LangTyp2.gif \"credit: Mark Riedl\")\n",
    "\n",
    "> Many LLMs like ChatGPT can take in 4,000 tokens (words), not just 3 as illustrated above. ***Clearly, they must have found a better solution than our Neural Network set-up.***\n",
    "\n",
    "#### We will break-up the problem into two parts (or circuits in our Neural Network):\n",
    "* **Encoder** part (capture Input)\n",
    "* **Decoder** part (generate Output)"
   ],
   "metadata": {
    "id": "lyBtskcIszJp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 Encoders in Language Models\n",
    "\n",
    "#### Consider the the following sentences that are each missing a word:\n",
    "1. The king sat on the ___\n",
    "2. The queen sat on the ___\n",
    "3. The princess sat on the ___\n",
    "4. The regent sat on the ___\n",
    "\n",
    "> My guess is as good as yours:\n",
    " * Maybe ***throne***?\n",
    "\n",
    "#### Do I really need separate wires between royals and throne?\n",
    "* king and throne\n",
    "* queen and throne\n",
    "* princess and throne\n",
    "* regent and throne\n",
    "\n",
    "#### **Idea**: Use an intermediate \"*concept*\" that approximately means \"*royalty*\"\n",
    "* Every time we see king or queen (etc.) we use this intermediate \"concept\"\n",
    "* Enough to know what to do when this intermediate \"concept\" appears:  \n",
    "  ==> **Send a strong signal to \"throne\"** *(i.e., its associated Neuron)*.\n",
    "\n",
    "\n",
    "#### Let's **Operationalize** the Idea in a Neural Network\n",
    ">* Set-up Input Layer with 50,000 Neurons (one for each word/token)\n",
    ">* Set-up an intermediate (hidden) layer with only 256 Neurons\n",
    "    \n",
    ">* No longer try to trigger just one Neuron (single striker in former output layer of 50,000 Neurons)\n",
    ">* We mash things up by triggering multiple Neurons on the intermediate layer of 256 Neurons\n",
    "\n",
    ">* Each possible combination of triggered Neurons could represent a differnt \"concept\" (like royalty or idian food or hoofed mammals).\n",
    ">* 256 Neurons allow us to represent $2^{256} = 1.15 \\times 10^{78}$ \"concepts\".\n",
    "\n",
    "![NN Language Typewriter](https://mapXP.app/MBA742/MRiedl_LangTyp3.gif \"credit: Mark Riedl\")\n",
    "  \n",
    "* We can capture even more \"concepts\" when we consider the \"intensity\" by which each Neuron gets triggered\n",
    "  * In my driving experience, alternating between flooring it or keeping the foot entirely of the accelerator or brakes (Known as [bang-bang control](https://en.wikipedia.org/wiki/Bang%E2%80%93bang_control)) is not very wise.\n",
    "  * Instead of binary values [0,1], we can use decimals (floats) from -1 to 1 (e.g., 0.4) for triggering each of the 256 Neurons\n",
    "  * When the model gets a word (token) as input, it triggers 256 Neurons, each with a different \"intensity\".\n",
    "\n",
    "#### Summing up\n",
    "* Before, 1 word (token) required 1 of 50,000 Neurons (strikers) to be triggered (rest remains passive).\n",
    "* Now, 1 triggered Neuron (striker) and 49,999 passive Neurons are reduced to 256 numbers (of varying magnitude) that capture a \"concept\"\n",
    "  * For the word \"king\" it might be   [-0.2, **0.3**, ..., 0.0, 0.6]\n",
    "  * For the word \"queen\" it might be [-0.2, **0.4**, ..., 0.0, 0.6].\n",
    "  \n",
    "> We call these *vectors* of 256 numbers ***Embeddings***.  \n",
    "> Key trick: create them by exposing some of the ***hidden state*** inside a neural network.\n",
    "\n",
    "---\n",
    "#### We call the neural network that compresses the 50,000 Neurons (typewriter keys, each representing a word) into 256 Neurons an **Encoder**.\n",
    "---"
   ],
   "metadata": {
    "id": "pURnYJtU9OV9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Decoders in Language Models\n",
    "\n",
    "The **Encoder** stops with the **Embeddings** that capture *latent* \"concepts\" from the input.   \n",
    "> *Latent* because we do not directly create or interpret them: Embeddings are vectors of numbers generated by the Neural Network.  \n",
    "\n",
    "The **Encoder** reduces words to latent concepts, but does not predict which word most likely comes next.\n",
    "\n",
    "> We need something that predicts words based on the states (values for each) of the 256 Neurons in that intermediate layer.\n",
    "\n",
    "The **Decoder** does exactly that:\n",
    "* Uses the ***Embedding*** generated by the ***Encoder*** (vector of 256 values)  \n",
    "* to ***trigger the original 50,000 Neurons*** of the output layer (strikers of the typewriter, one for each word/token),  \n",
    "* allowing us to ***pick the word*** (token) associated to the Neuron in the output layer (striker) ***with the strongest signal***.\n",
    "\n",
    "\n",
    "![NN Language Typewriter](https://mapXP.app/MBA742/MRiedl_LangTyp4.gif \"credit: Mark Riedl\")\n"
   ],
   "metadata": {
    "id": "o7TWZXs3_rGx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Language Model with Encoder and Decoder\n",
    "\n",
    "* We can ***connect*** Encoder and Decoder through that intermediate (hidden) layer of 256 Neurons\n",
    "* One big Neural Network\n",
    "  * receives word(token) as input,\n",
    "  * encodes 50,000 inputs in (hidden) layer of 256 Neurons (vector of 256 numbers = embedding) that capture latent \"concepts\"\n",
    "  * decodes \"concepts\" (hidden states of 256 Neurons = values = embedding) to output layer of 50,000 Neurons\n",
    "* Use output layer to predict word (token): that with strongest signal\n",
    "\n",
    "![NN Language Typewriter](https://mapXP.app/MBA742/MRiedl_LangTyp5.jpg \"credit: Mark Riedl\")\n",
    "\n",
    "\n",
    "---\n",
    "**Previously** required 50,000 x 50,000 = **2.5 billion** parameters (wires)\n",
    "\n",
    "**Now** only require 2 x (50,000 x 256) = **25.6 million** parameters (wires)\n",
    "\n",
    "---\n",
    "\n",
    "#### How does the Encoder-Decoder Language model know:\n",
    "* the signal strengths in the output layer\n",
    "* conditional on the input word (token)\n",
    "* so we can get $P \\ (\\text{W}_{out} \\,|\\, \\text{W}_{in})$ ?\n",
    "\n",
    "#### Need to ***adjust weights and biases*** of the network (encoder and decoder): Train the encoder-decoder neural network on text."
   ],
   "metadata": {
    "id": "lw9UG6pL_rJU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Training Language Models via Self-Supervised Learning\n",
    "\n",
    "#### **How to find the right embedding** (vector of 256 values) for every word (token) in our vocabulary (of 50,000 words) such that:\n",
    "* embredding for \"king\" is similar to \"queen\",\n",
    "* but quite different from \"goat\"\n",
    "\n",
    "#### Let's **start with a simple problem**\n",
    "* Encoder-Decoder neural network\n",
    "* accept a single word (token) as input\n",
    "* to produce the exact same word (token) as output\n",
    "\n",
    "![NN Language Typewriter](https://mapXP.app/MBA742/MRiedl_LangTyp6.gif \"credit: Mark Riedl\")\n",
    "\n",
    "1. Send the word \"king\" into the neural network\n",
    "2. Encoder creates embedding vector of 256 values in the middle\n",
    "3. Decoder sends strongest signal to same word \"king\" based on the embedding\n",
    "\n",
    "> **No guarantee that \"king\" gets strongest signal**\n",
    "> * Maybe \"goat\" gets a stronger signal (or probability) than \"king\".\n",
    "> * We actually don't care about the signal to \"goat\"\n",
    "> * Look at signal to \"king\" and see that it is not max (i.e., probability of 1.0)\n",
    "> * **How big is the error?** |Expected signal strength - actual signal strength| = ***Loss***\n",
    "\n",
    "#### Need to minimze ***Loss*** through training\n",
    "* Use ***BACKPROPAGATION***\n",
    ">  Backpropagation is a training algorithm used to optimize the weights and biases by minimizing the difference between the network's predicted output and the actual target values.\n",
    "* Consider all words (tokens) of vocabulary during training (i.e., minimze loss across all 50,000 words)\n",
    "  * Encoder ***must compromise*** because 256 neuron hidden layer is much smaller than vocabulary\n",
    "  * Some words may end up with very similar (or even same) ***embeddings***\n",
    "\n",
    "#### Good enough outcomes\n",
    "* Embeddings of \"king\" and \"queen\" become be very similar\n",
    "* Embeddings of \"king\" and \"queen\" become different from \"goat\"\n",
    "\n",
    "> Gives **Decoder** *a better chance to send the strongest signal to the correct word* (its associated Neuron) based on the 256 dimensional embedding.\n",
    "\n",
    "> We accept that \"king\" gets a signal with strength 0.62 and \"queen\" with 0.61\n",
    ">  * as long as all other 49,998 words (their Neurons) get a weaker signal.\n",
    "* In other words, we are probably going to be okay when our Language Model confuses kings and queens as long as it doesn't get confused between kings and goats.\n",
    "\n",
    "#### Self-supervision via same text\n",
    "\n",
    "> Our Language Model is **self-supervised** because it ***does not require separate outcome data*** (like the car example) for testing its output.  \n",
    "\n",
    "> **For training**, all it needs to do is **compare its output to its input**, *which is the same text (i.e., word sequence)*.\n"
   ],
   "metadata": {
    "id": "nZCKJPVk_rL9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Masked Language Models (MLMs)\n",
    "\n",
    "#### **Basic Idea**:\n",
    "* Take in a sequence of words (tokens) and generate a sequence of words (tokens).\n",
    "* One (or more) of the words are randomly blanked out, that is, [MASKED].\n",
    "* Predict the original [MASKED] word (token) in the output sequence, *based solely on its context* (i.e., the other words in the sequence)\n",
    "* Training involves adjusting weights and biases of neural network (using backpropagation) to minimze loss in ***masked token prediction***\n",
    "\n",
    "> Approach is widely used for ***pre-training*** language models\n",
    "> * Model is trained on a very large corpus of general text (computationally very expensive)\n",
    "> * Pre-trained model can then be adapted to different tasks without needing to be re-trained from scratch (we call this ***fine-tuning***)\n",
    "> * Fine-tuning makes a few updates to the pre-trained model to specialize it for a task or domain (more about this in the upcoming lecture on Fine-Tuning LLMs).\n",
    "> * ***BERT*** (Bidirectional Encoder Representations from Transformers) is one of the most well-known examples of a pre-trained MLM.\n",
    "\n",
    "\n",
    "\n",
    "![NN Language Typewriter](https://mapXP.app/MBA742/MRiedl_LangTyp7.jpg \"credit: Mark Riedl\")\n",
    "\n"
   ],
   "metadata": {
    "id": "kDDIn2Q5_rOl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Generative MLM\n",
    "\n",
    "* Special case of a masked language model that always masks the last word (token) for a sequence.\n",
    "* Called ***generative model*** because it is trained to predict (or generate) only the next word (token)\n",
    "* This is different from bidirectional models, like BERT, which can predict a word (token) at any position on a word sequence\n",
    "\n",
    ">#### The [MASK]\n",
    ">#### The king [MASK]\n",
    ">#### The king sat [MASK]\n",
    ">#### The king sat on [MASK]\n",
    ">#### The king sat on the [MASK]\n",
    "\n",
    "* Also referred to as an ***auto-regressive model***\n",
    "  * auto = self\n",
    "  * ***self-predictive model***\n",
    "\n",
    "#### **Generative process**:\n",
    "1. model predicts the next word from the previous word sequence\n",
    "2. the predicted word is added to end of sequence\n",
    "3. the subsequent word is predicted from updated sequence, and so on.\n",
    "\n",
    "> #### GPT (***Generative Pre-trained Transformer***) is a generative MLM.\n",
    "\n"
   ],
   "metadata": {
    "id": "wvf7vgEq_rRQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Transformer Models\n",
    "\n",
    "> #### A **transformer** is a deep learning model that transforms the encoding in a particular way to make predicting the [MASKED] word easier.\n",
    "\n",
    "* Introduced in the paper \"[Attention is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\" (Vaswani et al., 2017) by Google researchers.\n",
    "* A deep learning model for NLP that revolutionized how sequences are processed.\n",
    "\n",
    "#### **Core Architecture:**\n",
    "* Based on an ***encoder-decoder*** architecture.\n",
    "* Enhances encoding with ***self-attention***, *allowing each input part to relate to selected others.*\n",
    "\n",
    "#### **Decoder:**\n",
    "\n",
    "* Generates output using encoded data and self-attention, ***focusing on relevant input parts for each output step***.\n",
    "\n",
    "#### **Significance:**\n",
    "\n",
    "* ***Parallel Processing***: Unlike previous language models (e.g., recurrent neural networks; RNNs and their variants), Transformers process entire sequences simultaneously, making them more efficient and suitable for parallel computation.\n",
    "* ***Context-Awareness***: Transformers capture deeper understanding of the context and relationships within the data, significantly improving performance on tasks like translation, question-answering, and text summarization.\n",
    "\n",
    "![Transformer Architecture](https://mapXP.app/BUS488/insideTransformers \"credit: Daniel Ringel, 2024\")"
   ],
   "metadata": {
    "id": "SASYjm-qz6J7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Self-Attention\n",
    "\n",
    "> #### Self-attention is a mechanism within the Transformer model that enables each word in a sentence to dynamically influence and be influenced by other words, capturing their interdependencies regardless of their positions.\n",
    "\n",
    "![NN Language Typewriter](https://mapXP.app/MBA742/MRiedl_LangTyp8.jpg \"credit: Mark Riedl\")\n",
    "\n",
    "#### **The Idea of Self-Attention**:\n",
    "Imagine the sentence:  \n",
    "\n",
    "      The alien landed on earth because it needed to hide on a planet.\n",
    "\n",
    "* If some words were missing, you'd guess them based on the other words.\n",
    "* Self-attention works similarly: It helps the model figure out how words in a sentence relate to each other (to \"understand\" the full context).\n",
    "\n",
    "#### **How It Works**:\n",
    "* If *alien* were missing, the model ***uses clues from related words*** like *landed* and *earth* to guess it.\n",
    "* Similarly, if *it* were missing, knowing that *alien* is previously mentioned helps the model decide that *it* likely refers to *alien*, not *earth* or *planet*.\n",
    "* This process allows the model to \"understand\" which words are important to each other in a sentence.\n",
    "\n",
    "#### **Contextualized Word Embeddings**:\n",
    "* For each word, the model creates a special, new blend of numbers (i.e., a contextualized embedding)\n",
    "  * captures not just the word itself,\n",
    "  * but also its connection to other words.  \n",
    "\n",
    "  ***Example:*** By combining the numeric representations (word embeddings) of “alien”, “landed”, and “earth”,   \n",
    "  the model creates a new, enriched (contextualized) embedding.\n",
    "* New (contextualized) embedding doesn't directly match any single word.\n",
    "* New (contextualized) embedding carries pieces of all related words, giving a fuller picture of the sentence.\n",
    "\n",
    "#### **Why It Matters**:\n",
    "* Enriched understanding helps the model make better predictions\n",
    "(especially when filling in missing words).\n",
    "* The model sees not just the words themselves, but also how they fit together in the story.\n",
    "* When the model encounters a blank (like in a fill-in-the-blank question), it uses the whole sentence's context, not just guesswork.\n",
    "\n",
    "#### **Practical Takeaway**:\n",
    "* Think of self-attention as the model's way of reading between the lines\n",
    "   * using the context provided by all words\n",
    "   * to understand each word's role and meaning better.\n",
    "* This is crucial for tasks like translating languages or answering questions where understanding context and nuances is key."
   ],
   "metadata": {
    "id": "JsMoRdp-_rTt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.1 Calculating Self-Attention with Queries, Keys and Values\n",
    "*without the math*\n",
    "\n",
    "**Step 1**: Imagine every word in a sentence gets\n",
    "* a special set of ***glasses*** (called queries),\n",
    "* a name ***tag*** (called keys),\n",
    "* and a ***backpack*** (called values).\n",
    "\n",
    "**Step 2**:\n",
    "* Each word looks at every other word through its ***glasses***.\n",
    "* How clearly it sees another word's ***name tag*** tells it how important that word is to it.\n",
    "\n",
    "**Step 3**:\n",
    "* Based on importance, each word takes a little from the others' ***backpacks***\n",
    "* The more important a word is, the more it takes.\n",
    "\n",
    "**Step 4**:\n",
    "* Every word ends up with a mix of stuff from all the other ***backpacks***,\n",
    "* weighted by importance (how clearly a word sees other words' ***name tags*** through its ***glasses***).\n",
    "* This mix is the new, improved version of the word, considering the whole sentence.\n",
    "\n",
    "---\n",
    "\n",
    "* **Queries** are like glasses that help see how relevant other words are.\n",
    "* **Keys** are like name tags that show a word's identity.\n",
    "* **Values** are like backpacks carrying the word's content.\n",
    "  > The mix each word gets is a smarter version of itself, now informed by the whole sentence.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Dot Product Attention**:\n",
    "* The self-attention mechanism's ability to assess the importance of words relative to each other is often referred to as ***dot product attention***.\n",
    "* This name is derived from the specific mathematical operation it employs:\n",
    "  * At its core, the model calculates the ***dot product*** between the query vector of one word and the key vector of every other word.\n",
    "  * The ***dot product*** multiplies corresponding values of two vectors and sums to produce a number that serves as a measure of similarity or relevance.\n",
    "  * The reason it's called ***dot product attention*** is because this operation directly influences how much focus (or attention) is given to words in the sequence.\n",
    "  * A ***higher dot product indicates greater relevance or similarity***, guiding the model to pay more attention to certain words when constructing the contextualized embedding of each word.\n",
    "\n",
    "#### **Implications of Dot Product Attention**:\n",
    "  * Allows for a dynamically weighted representation of the sentence,\n",
    "  * where each word's influence on another\n",
    "  * is quantified by their dot product.\n",
    "\n",
    "> Dot Product Attention is a crucial aspect that enables the Transformer to understand and represent the nuanced interplay of words within a sentence.\n",
    "\n",
    "**NOTE**: The dot product is part of the calculation for cosine similarity. Cosine similarity normalizes the dot product by the magnitudes of the vectors, focusing solely on the directionality (angle) rather than the magnitude. (*Ok, there is math, but I did it without the calculation.*)"
   ],
   "metadata": {
    "id": "NSRrre1LH__T"
   }
  }
 ]
}
