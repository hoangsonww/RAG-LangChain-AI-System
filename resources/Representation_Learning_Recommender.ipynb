{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkUYF0gHmDP_"
   },
   "source": [
    "# Representation Learning for Recommender Systems\n",
    "\n",
    "Technologies Used:\n",
    "\n",
    "![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)\n",
    "![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)\n",
    "![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)\n",
    "![Numpy](https://img.shields.io/badge/Numpy-013243?style=for-the-badge&logo=numpy&logoColor=white)\n",
    "![Matplotlib](https://img.shields.io/badge/Matplotlib-013243?style=for-the-badge&logo=matplotlib&logoColor=white)\n",
    "![Seaborn](https://img.shields.io/badge/Seaborn-013243?style=for-the-badge&logo=seaborn&logoColor=white)\n",
    "![Scikit-Learn](https://img.shields.io/badge/Scikit_Learn-013243?style=for-the-badge&logo=scikit-learn&logoColor=white)\n",
    "![Gensim](https://img.shields.io/badge/Gensim-013243?style=for-the-badge&logo=gensim&logoColor=white)\n",
    "![Altair](https://img.shields.io/badge/Altair-013243?style=for-the-badge&logo=altair&logoColor=white)\n",
    "![Google Colab](https://img.shields.io/badge/Google_Colab-013243?style=for-the-badge&logo=google-colab&logoColor=white)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSyEjFHp3z4u"
   },
   "source": [
    "# 1. Word Embeddings\n",
    "Word embeddings are ***vectors of numbers*** (row of real valued numbers) that ***represent*** the ***meaning*** of a ***word*** from the contexts it appears in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szimL6XevQgE"
   },
   "source": [
    "## 1.1 World without Word Embedding\n",
    "**The problem of understanding text is not new:**\n",
    "\n",
    "* Machine learning models for text used ***hand crafted features*** from data.   \n",
    "    * **Bag-of-Words:** The count of tokens (words, [n-grams](https://books.google.com/ngrams/graph?content=tar+heel%2Cblue+devil&year_start=1770&year_end=2008&corpus=en-2009&smoothing=3))\n",
    "    * **TF-IDF:**  Term frequency-inverse document frequency is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. Works by increasing proportionally to the number of times a word appears in a document but is offset by the number of documents that contain the word.  \n",
    "\n",
    "* ***Syntactic relationships*** in text. **Syntax** is the study of sentence structure and the rules of grammar.\n",
    "    - POS tags\n",
    "    - Stemming\n",
    "    - Lemmatization   \n",
    "\n",
    "* ***Probabilistic Topic Modeling*** reveals hidden thematic structure of documents.\n",
    "    - Latent Dirichlet allocation (**LDA**) is a generative model for collections of discrete data such as text corpora.\n",
    "    - Represents documents as a bag-of-words\n",
    "    - Document structure is neglected\n",
    "\n",
    "**Challenge**\n",
    "\n",
    "* ***Semantics***: Features like word count do not actually capture the meaning of a word or the context in which it is used, that is, the semantics. ***Semantics*** is the study of the meaning of sentences.\n",
    "\n",
    "**Idea**\n",
    "* Word Embeddings ***encode the meaning and context*** of words as a vector of numbers (because ML models understand only numbers).\n",
    "\n",
    "* Word Embeddings are ***not defined by humans*** but are ***learned by a ML model*** during training.\n",
    "* No need to handcraft features; just need a large corpus of text data.\n",
    "\n",
    "* **Important:** Embedded features are ***latent***, that is, they are not directly observable and interpretable to us!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxEcWfq4n-9x"
   },
   "source": [
    "## 1.2 What are Word Embeddings?\n",
    "\n",
    "* Word embeddings map each word to a vector (a row of $d$ real-valued numbers) that represent the meaning of a word based on the contexts it appears in.   \n",
    "\n",
    "* The number $d$ is the *dimension* of the vector space that the words are embedded in.  \n",
    "\n",
    "\n",
    "\n",
    "* Word vectors lend themselves to mathematical operators:\n",
    "  - Addition: columnwise\n",
    "  - Subtraction: \"\n",
    "  - Averaging: \"\n",
    "  - Euclidean distance: square root of sum of squared differences, as usual\n",
    "  - Cosine similarity: sum of columnwise products; related to angle between vectors\n",
    "\n",
    "* Semantically similar words (i.e., words that are used in a similar context) will have similar vectors.   \n",
    "\n",
    "* Likewise, short distance between two vectors = semantic similarity.  \n",
    "\n",
    "***For Example:***\n",
    "\n",
    "${WV(king) — WV(man) + WV(woman) \\approx WV(queen)}$\n",
    "\n",
    "- Subtract one meaning from the word vector for king (i.e., \"maleness\"), add another meaning (i.e., \"femaleness\")\n",
    "- Show that this new word vector (king — man + woman) is most similar to the word vector for queen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No8OjDO1vhkZ"
   },
   "source": [
    "### 1.2.1 What does a Word's Vector look like?\n",
    "\n",
    "This is a word vector for the word “king”\n",
    "\n",
    "${[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]}$\n",
    "\n",
    "- Here, vector of length $d=50$\n",
    "- Embedding length is a hyperparameter ...  \n",
    "  ... of how many dimensions to embed words into\n",
    "- Vector length depends on a few factors like:\n",
    "  * vocabulary size\n",
    "  * computational resources\n",
    "- No straightforward formula to calculate \"correct\" embedding length\n",
    "- For large vocabulary typically length of 300 (dimensions = columns = numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I79gUUOCtsJC"
   },
   "source": [
    "## 1.3 Visual Inspection of Word Vectors\n",
    "- Below we consider word embeddings of length 50\n",
    "- We create a 1-dimensional heatmap for the values of each vector dimension\n",
    "  - red if they’re close to 2\n",
    "  - white if they’re close to 0\n",
    "  - blue if they’re close to -2\n",
    "  - shades of red and blue for values between 0 and 2 as well as -2 and 0, respectively\n",
    "\n",
    "Let’s now contrast “King” against other words:\n",
    "\n",
    "![](https://www.mapxp.app/BUSI488/vector-similarity1)  \n",
    "[image source](https://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "- See how “Man” and “Woman” are much more similar to each other than either of them is to “king”?\n",
    "- Tells us something: Vector representations capture the meaning of words\n",
    "- The meaning of a word is embedded as latent (i.e., hidden) features in the word's vector\n",
    "\n",
    "Here’s another list of examples (compare by vertically scanning the columns looking for columns with similar colors):\n",
    "\n",
    "\n",
    "![](https://www.mapxp.app/BUSI488/vector-similarity2)\n",
    "[image source](https://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "1. There’s a straight red column through all of these different words. They’re similar along that dimension (and we don’t know what each dimension codes for).  \n",
    "\n",
    "2. You can see how “woman” and “girl” are similar to each other in a lot of places. The same with “man” and “boy”.  \n",
    "\n",
    "3. “boy” and “girl” also have places where they are similar to each other, but different from “woman” or “man”. Could these be coding for a vague conception of youth? Possibly. We cannot know for certain!  \n",
    "\n",
    "4. All but the last word represent people.  \n",
    "\n",
    "5. We added the word vector for \"water\" to show the differences between word's vectors relative to their meaning. You can, for example, see that blue column going all the way down and stopping before the vector of “water”  \n",
    "\n",
    "5. There are several dimensions (i.e., columns) where “king” and “queen” are similar to each other and distinct from the other words' vectors. Could these be coding for a vague concept of royalty? Possibly, but we cannot know for certain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnJOna9P2Zdh"
   },
   "source": [
    "## 1.4 Visualize the Semantic Similarity of Words\n",
    "-\tWords that are similar or appear in similar contexts\n",
    "-\tWord with similar context have similar word vectors\n",
    "Let’s do some visual discovery of word similarities!\n",
    "\n",
    "**For this visualization of word vectors**\n",
    "\n",
    "* We will use a popular word2vec Python package called ***gensim***\n",
    "\n",
    "* We will ***load a Google Pre-trained Word2Vec model*** from a publicly available google drive\n",
    "\n",
    "* By pre-trained we mean that the model has already been trained on some data and has learned the word vector representations.\n",
    "\n",
    "* Remember that Word Embeddings are not defined by humans but are learned by an ML model during training.\n",
    "\n",
    "* We will obtain the word embeddings for some words and visualize how similar they are using t-SNE.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_sSBn2ImlD1v"
   },
   "source": [
    "# 0. Connect your Google Drive (ONLY USE THIS IF YOU ARE USING GOOGLE COLAB!)\n",
    "\n",
    "# a. Import required package\n",
    "from google.colab import drive\n",
    "\n",
    "# b. Mount drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# c. Modify the path if necessary\n",
    "%cd /content/gdrive/MyDrive/488/Class05\n",
    "\n",
    "# d. List the files that are in the current folder\n",
    "!ls # special shell command to view the files in the home directory of the notebook environment"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 0. Check how much RAM we have available\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ],
   "metadata": {
    "id": "n05L9uXCV8pz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rwOrjor41K3X"
   },
   "source": [
    "# 1. Import some packages we will need\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from lxml import etree\n",
    "import glob, csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6tShEF579VGx"
   },
   "source": [
    "%%time\n",
    "# 2a. Define name of the pretrained embedding file we will use\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300-500k.bin.gz'\n",
    "\n",
    "# 2b. Load embedding from the file - will take a moment!\n",
    "google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "# The original google news pre-trained model has a vocabulary of over 3 billion words, sorted by frequency\n",
    "# I only used the first 500k words here to save RAM"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "##### Other available models: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "### Load as follows (but count on waiting a long time for large models to download)\n",
    "# import gensim.downloader\n",
    "\n",
    "# # Show all available models in gensim-data\n",
    "# print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "# # Load a full model (takes a long time!)\n",
    "# google_word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ],
   "metadata": {
    "id": "--biRQCW6QlV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W-svfkB85cPc"
   },
   "source": [
    "# 3. Create a string of words we are interested in\n",
    "words = \"man woman boy girl king queen water\"\n",
    "#words = \"apple banana orange pear tomato onion cucumber carrot\"\n",
    "#words = \"apple google facebook ibm microsoft samsung LG amazon\"\n",
    "#words = \"tomato onion cucumber carrot banana orange pear apple google facebook ibm microsoft samsung LG amazon\"\n",
    "\n",
    "# 4. Turn the string into individual tokens\n",
    "word_tokens = words.split(' ')\n",
    "\n",
    "# 5. Get word vectors from pre-trained word embedding for all tokens of our string\n",
    "word_vectors = [google_word2vec[x] for x in word_tokens]\n",
    "#df = pd.DataFrame(google_word2vec.get_normed_vectors()[word_vectors], index=word_vectors)\n",
    "\n",
    "# 6. Show how long the vectors are\n",
    "print(f\"Dimensions: {len(word_vectors[0])}\")\n",
    "\n",
    "# 7. Show the first 10 \"columns\" for the word 'man'\n",
    "print(f\"First 10 dimensions of the word embedding for '{word_tokens[0]}': {word_vectors[0][:10]}\")\n",
    "\n",
    "# 8. Visualize Word Vectors\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style({\"ytick.color\": \"red\"})\n",
    "f, ax = plt.subplots(figsize=(40, 10))\n",
    "sns.heatmap(word_vectors, cmap='coolwarm', xticklabels=False, yticklabels=word_tokens, cbar=False, robust=True, linewidth=.5)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAYOHn6phIhf"
   },
   "source": [
    "### 1.4.1 Dimensionality Reduction with t-SNE\n",
    "\n",
    "- We can use t-SNE to reduce the dimensionality of the word vectors from 300 dimensions to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "word_vectors[0]"
   ],
   "metadata": {
    "id": "kHPABY8gI_FR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hbmpfxOphFn0"
   },
   "source": [
    "# 1. Import required package\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 2. Set value for perplexity: we have very few objects (i.e., words), therefore we set a very low value\n",
    "perp = 3  # perplexity of similarity kernel\n",
    "\n",
    "# 3. Set multiple Seeds for random initializations\n",
    "sds=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n",
    "KL = []\n",
    "\n",
    "# 4. Find best seed (i.e., lowest KL divergence)\n",
    "for s in sds:\n",
    "    tsne = TSNE(n_components = 2, perplexity = perp, init='random', learning_rate=100, random_state=s)\n",
    "    X_tsne = tsne.fit_transform(np.array(word_vectors))\n",
    "    KL.append(tsne.kl_divergence_)\n",
    "    print(tsne.kl_divergence_, end=\" \")\n",
    "\n",
    "# 5. Fit Best Model\n",
    "Y = TSNE(n_components = 2, perplexity = perp, init='random', learning_rate=100, verbose=1, random_state=sds[KL.index(min(KL))]).fit_transform(np.array(word_vectors))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D099LPD3kkww"
   },
   "source": [
    "### 1.4.2 Visualization in an Interactive Map with Altair\n",
    "\n",
    "\n",
    "- We can now visualize how similar words are based on their latent meanings on a map by treating the 2 dimensions of t-SNE as X and Y coordinates.\n",
    "\n",
    "Altair is a powerful tool for interactive visualization in Python https://altair-viz.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vrt1r_XvhFrE"
   },
   "source": [
    "# 1. Create a new DataFrame that holds all the information we need for our map\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(\n",
    "    {'x': Y[:,0],\n",
    "     'y': Y[:,1],\n",
    "     'txt': words.split()\n",
    "    })\n",
    "\n",
    "# 2. Import Altair\n",
    "import altair as alt\n",
    "\n",
    "# 3. Plot the dataset by referencing DataFrame column names\n",
    "base = alt.Chart(df).encode(\n",
    "    x=alt.X('x', axis=alt.Axis(title=\"X\", grid=True, labels=False), scale=alt.Scale(domain=[-500, 525])),\n",
    "    y=alt.Y('y', axis=alt.Axis(title=\"Y\", grid=True, labels=False), scale=alt.Scale(domain=[-300, 300]))\n",
    ")\n",
    "\n",
    "base.mark_circle(size=100) + base.mark_text(dx=25,fontSize=12).encode(text='txt').interactive().properties(height=600,width=600,title='Similarity of Word Vectors')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLPIhZas7QP2"
   },
   "source": [
    "- The words `man` and `woman` are close together\n",
    "  - and are also close to `boy` and `girl`.\n",
    "- The words `king` and queen are close to each other.\n",
    "- All words are further away from `water`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjN75NE0hIkm"
   },
   "source": [
    "### 1.4.3 Heat Map of Word Vectors\n",
    "\n",
    "-\tThe value of representing words as word vectors comes from being able to compare words to see how similar they are.\n",
    "-\tWhen dealing with vectors, a common way to calculate a similarity score is cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tg6I8uLwhP29"
   },
   "source": [
    "# 1. Import required packages\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "# 2. Calculate Cosine Similarity for first word (man) - optional\n",
    "cos_sim = cosine_similarity(word_vectors, np.tile(word_vectors[0], (7, 1)))[:, 1]\n",
    "\n",
    "# 3. Generate Cosine Similarity Matrix\n",
    "sim_matrix = cosine_similarity(word_vectors, word_vectors)\n",
    "\n",
    "# 4. Get actual words\n",
    "word_labels = [str(word) for word in word_tokens]\n",
    "\n",
    "# 5. Generate Heat Map\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(sim_matrix, annot=True, xticklabels=word_labels, yticklabels=word_labels, cmap=\"YlGnBu\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. Free up some RAM by deleting what we don't need\n",
    "word_vectors=[]\n",
    "sim_matrix=[]\n",
    "word_labels=[]\n",
    "cos_sim=[]\n",
    "base=[]\n",
    "df=[]\n",
    "tSNE=[]\n",
    "Y=[]\n",
    "KL=[]\n",
    "ax=[]"
   ],
   "metadata": {
    "id": "cLIvW8JLziCx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkPcRYg712Is"
   },
   "source": [
    "## 1.5 Operations on Embeddings\n",
    "\n",
    "Since word vectors are numbers, we can perform operations like addition, subtraction and averaging.   \n",
    "\n",
    "- What is the motivation behind doing this?\n",
    "- What's going on under the hood?\n",
    "- Let's take a look!\n",
    "\n",
    "**Example 1**\n",
    "In the example we had when introducing word embeddings, we said that we could so something like this\n",
    "\n",
    "$WV(king) — WV(man)+WV(woman)=WV(queen)$\n",
    "\n",
    "That is, subtract one meaning from the word vector for king (i.e. maleness), add another meaning (femaleness) and that this new word vector (king — man + woman) is most similar to the word vector for queen.\n",
    "\n",
    "![Word Embedding Arithmatic](https://www.mapxp.app/BUSI488/VisualWordArithmatic2.jpg \"Word Embedding Arithmatic\")\n",
    "\n",
    "\n",
    "\n",
    "Let's take a look at this in code"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SguHw2lc6rX1"
   },
   "source": [
    "# 1. Define Words (tokens)\n",
    "words = [\"king\", \"man\", \"woman\"]\n",
    "#words = [\"google\", \"amazon\", \"LG\"]\n",
    "\n",
    "# 2. Get vectors from pre-trained model\n",
    "word_vectors = [google_word2vec[x] for x in words]\n",
    "\n",
    "# 3. Do the math! result = king - man + woman\n",
    "result = word_vectors[0] - word_vectors[1] + word_vectors[2]\n",
    "\n",
    "# 4. Most similar words to the result (note that as a heuristic, you might need to ignore the first, that is, most similar result)\n",
    "google_word2vec.most_similar(positive=[result], topn=5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. Append Calcualted Word Vector and Word\n",
    "word_vectors.append(result)\n",
    "words.append(\"King-Man+Woman\")\n",
    "\n",
    "# 6. Visualize Word Vectors\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style({\"ytick.color\": \"red\"})\n",
    "f, ax = plt.subplots(figsize=(40, 10))\n",
    "sns.heatmap(word_vectors, cmap='coolwarm', xticklabels=False, yticklabels=words, cbar=False, robust=True, linewidth=.5)\n",
    "#plt.yticks(rotation=0)"
   ],
   "metadata": {
    "id": "fbLqd9DbZTQg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjfohf2yMXa8"
   },
   "source": [
    "From the above example, we see that on subtracting `man` and adding `woman` to `king` we get a result that is close to `queen` (ignoring the query words itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ASudUERoDVz"
   },
   "source": [
    "**Example 2:** We have 3 brothers (Huey, Dewey, and Louie) who like different sets of animals.\n",
    "\n",
    "![(c) www.disneyclips.com](https://www.disneyclips.com/characters/huey-dewey-louie.gif \"Huey, Dewey, and Louie\")\n",
    "\n",
    "\n",
    "*We want to know how similar their preferences for animals are!*\n",
    "\n",
    "1. Average the word vectors of the animals each of them likes to get a single vector that represents their favorites\n",
    "\n",
    "2. Compare averaged vectors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o26GgzarD4Dw"
   },
   "source": [
    "# 1. Average vectors for Huey\n",
    "Huey_favorite_animals = [\"penguin\", \"bear\", \"seal\"]\n",
    "Huey_favorite_vectors = [google_word2vec[x] for x in Huey_favorite_animals]\n",
    "Huey_favorite = np.average(Huey_favorite_vectors, axis=0)\n",
    "\n",
    "# 2. Average vectors for Dewey\n",
    "Dewey_favorite_animals = [\"mouse\", \"rat\", \"chipmunk\"]\n",
    "Dewey_favorite_vectors = [google_word2vec[x] for x in Dewey_favorite_animals]\n",
    "Dewey_favorite = np.average(Dewey_favorite_vectors, axis=0)\n",
    "\n",
    "# 3. Average vectors for Louie\n",
    "Louie_favorite_animals = [\"whale\", \"shark\", \"dolphin\"]\n",
    "Louie_favorite_vectors = [google_word2vec[x] for x in Louie_favorite_animals]\n",
    "Louie_favorite = np.average(Louie_favorite_vectors, axis=0)\n",
    "\n",
    "# 4. Compare\n",
    "print(\"Similarity of of Huey's and Dewey's preferences for animals: \" , cosine_similarity([Huey_favorite, Dewey_favorite])[0][1])\n",
    "print(\"Similarity of of Huey's and Louie's preferences for animals: \" , cosine_similarity([Huey_favorite, Louie_favorite])[0][1])\n",
    "print(\"Similarity of of Dewey's and Louie's preferences for animals: \" , cosine_similarity([Dewey_favorite, Louie_favorite])[0][1])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. Visualize Mean Word Vectors in heatmaps\n",
    "favorits= np.stack((Huey_favorite,Dewey_favorite, Louie_favorite))\n",
    "borthers=[\"Huey\", \"Dewey\", \"Louie\"]\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style({\"ytick.color\": \"red\"})\n",
    "f, ax = plt.subplots(figsize=(40, 10))\n",
    "sns.heatmap(favorits, cmap='coolwarm', xticklabels=False, yticklabels=borthers, cbar=False, robust=True, linewidth=.5)\n",
    "plt.yticks(rotation=0)"
   ],
   "metadata": {
    "id": "Rv0nihiyxz_p"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4Og4Og0nD_3"
   },
   "source": [
    "In this section, we learned that:\n",
    "\n",
    "1. We can represent words as vectors of numbers.\n",
    "2. We can easily calculate how similar vectors are to each other and also perform other mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa2Tm8iEoaRG"
   },
   "source": [
    "# 2. Word2Vec\n",
    "\n",
    "**Word2Vec is a popular model to train and obtain word embeddings**\n",
    "\n",
    "* We just used a pre-trained Word2Vec model in our example above  \n",
    "\n",
    "* Word2Vec is a computationally efficient model composed of a **two-layer neural network** (i.e., a shallow neural network)  \n",
    "\n",
    "* The model ***learns word embeddings from raw text*** and is trained to reconstruct linguistic contexts of words.  \n",
    "\n",
    "* The model ***input*** is a large text corpus.\n",
    "\n",
    "* The final ***output*** that we are interested in is a higher-dimensional vector space (i.e., word embeddings) where ***each unique word in the text corpus is embedded into a vector in this space***.   \n",
    "\n",
    "* Words are embedded into word vectors such that\n",
    "   - ***words that share common contexts*** in the text corpus ***have similar word vectors***\n",
    "   - and are thus are ***located in close proximity*** to one another in the vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhRrAjVrLkLY"
   },
   "source": [
    "## 2.1 Training Data\n",
    "Training a Word2Vec model requires real-world text such that the model can learn which words appear in similar contexts.  \n",
    "\n",
    "**Major Advantage:** We do not need human labeled data.   \n",
    "\n",
    "We can train Word2Vec on various texts such as:\n",
    "- Books\n",
    "- News\n",
    "- Wikipedia content\n",
    "- Congressional debates\n",
    "- or any other ***abundant*** text data\n",
    "\n",
    "#### So how do we use such text data to train a word2vec model?\n",
    "\n",
    "1. We get a lot of text data (all Wikipedia articles, for example) and provide it to the model for training.  \n",
    "\n",
    "2. The model then analyzes the context of words via a so-called \"window\"\n",
    "   - the window contains words before and after a focal (i.e., input) word\n",
    "   - we can define how big this window should be (hyperparameter!) such as 2 words before and after the focal word.\n",
    "   - you can think of this window \"sliding\" or \"rolling\" across the text as the model trains.  \n",
    "\n",
    "3. The \"sliding\" window generates training samples for our model (i.e., words around a focal word are positive samples - these are our targets in skip-gram).\n",
    "\n",
    "4. The model then trains by trying to accurately predict the context words (i.e., targets) from a focal word (this approach is called skip-gram), or the focal word from its context (this approach is called CBOW), whereby the model corrects itself iteratively to make its predictions more and more accurate\n",
    "\n",
    "\n",
    "![Contexts and Windows in Skip-Gram](https://www.mapxp.app/BUSI488/NeuralWordEmbedding4.jpg \"Contexts and Windows in Skip-Gram\")\n",
    "\n",
    "**For Example:**\n",
    "* Given a sentence: The data-breach at Sony betrayed my trust.\n",
    "* The focal word: Sony\n",
    "* Window size = 2\n",
    "* Context words (i.e., targets) are: data-breach, at, betrayed, my\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blaiS38_6oBM"
   },
   "source": [
    "## 2.2 Model Architecture\n",
    "\n",
    "* Word2Vec is originally a **Shallow Neural Network** with a single hidden layer, and like all neural networks, it has weights which it adjusts during its training to reduce a loss function.\n",
    "\n",
    "* We feed words as one-hot vectors, which is basically a vector of the same length as the vocabulary, filled with zeros except at the index that represents the word we want to represent, which is assigned “1”.\n",
    "\n",
    "* The hidden layer is a standard fully connected (dense) layer whose weights are randomly initialized.\n",
    "\n",
    "* The output layer outputs probabilities for the target words (i.e., context words) from the vocabulary. Each output node (a specific word) is connected by N weights to N hidden neurons.\n",
    "\n",
    "* The N weights for a specific output node (word) is the word embedding or word vector for that word.\n",
    "\n",
    "Over the course of **training** the **weights are adjusted** and the **vectors of words that share similar contexts become more and more similar.** By doing this the model learns the task of what words appear in context of an input word.\n",
    "\n",
    "![Skip-Gram: Shallow Neural Network](http://www.mapxp.app/BUSI488/NeuralWordEmbedding3a.jpg \"Skip-Gram: Shallow Neural Network\")\n",
    "\n",
    "- When the feature vector assigned to a word ***cannot*** be used to accurately predict that word’s context, the components of the vector are adjusted (this is an iterative training process)\n",
    "- Each word's context in the corpus is the \"teacher\" that sends error signals back to adjust the feature vector\n",
    "- The feature vectors of words judged to be similar by their context are nudged closer together by adjusting the numbers in the vector\n",
    "- The neural network is shallow because it has only 3 layers:\n",
    "    - **Input layer** that serves as indicator for each word in the vocabulary\n",
    "    - **Hidden layer**, which is what we are ultimately interested in: a feature vector for each word that was learned to encode context.n\n",
    "    - **Output layer** that determines the context words from the feature vectors for each word: capture the probabilities that output words will be in close positions to a given word (i.e., the context of the word that is defined as a context window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsYRgUVqA-cU"
   },
   "source": [
    "**How are the weights updated to learn the word vectors?**\n",
    "* Learning is similar to a classification model.   \n",
    "\n",
    "* The model takes as input a word and multiplying with the weights **predicts an output probability distribution over all the words in the vocabulary**. That is, we have **V class labels** where V is the size of the vocabulary.   \n",
    "\n",
    "* This output probability is compared against the actual target word (i.e., the label) and the **error is computed**  \n",
    "\n",
    "* This error is used to **update the weights using a technique called backpropagation.**  \n",
    "\n",
    "* The weights are updated such that in the next iteration with this (input, label) pair the output error is lesser, that is, the prediction is closer to the target word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z2I2qQk_Zy3"
   },
   "source": [
    "**The \"Irony\" of Training Word2Vec**\n",
    "\n",
    "- Word2Vec is not used for the task it was trained on  \n",
    "\n",
    "- Actually want to learn the hidden layer weight matrix   \n",
    "\n",
    "- Throw away output layer  \n",
    "\n",
    "- Rows of the hidden layer weight matrix = Word vectors (word embeddings) we want!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAOnUZK4AggH"
   },
   "source": [
    "## 2.3 Skip-Gram with Negative Sampling (SGNS) Model\n",
    "\n",
    "- **Word2Vec** provides a variety of methods of training a model.\n",
    " - We will use Skip-Gram with Negative Sampling (**SGNS**)\n",
    " - Continuous Bag-of-Words (**CBOW**) model is another option   \n",
    "   \n",
    "\n",
    "\n",
    "- **Skip-gram** predicts surrounding context words from the target words.\n",
    "  - Number of surrounding words defined as hyperparameter (window)  \n",
    "\n",
    "\n",
    "- **Skip-Gram with Negative Sampling** improves computational efficiency substantially:\n",
    "\n",
    "  - Switch the model’s task\n",
    "    - from predicting a neighboring word\n",
    "    - to predicting whether a word pair (input and output) are neighbors (0 for “not neighbors”, 1 for “neighbors”)\n",
    "  - ***Changes model***\n",
    "    - *from* a shallow neural network\n",
    "    - *to* a logistic regression model\n",
    "  - Much simpler and much faster to calculate!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwUQxsnOo2TC"
   },
   "source": [
    "**Algorithm**  \n",
    "\n",
    "1. Treat an input (focal) word and a neighboring context word (target) as positive examples\n",
    "2. Randomly sample other words in the lexicon (i.e., vocabulary) to get negative samples\n",
    "3. Predict if neighbors or not, and calculate error\n",
    "4. Update feature weights of word pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjZiusf8np3s"
   },
   "source": [
    "###2.3.1 SGNS Step 1\n",
    "\n",
    "Generate positive samples (or examples) using a sliding window size = 2.  \n",
    "\n",
    "![](https://www.mapxp.app/BUSI488/sgns-1.jpg)\n",
    "\n",
    "\n",
    "For computational efficiency, ***we do not want to predict the target word (i.e., context words) directly!***\n",
    "\n",
    "Need **labels** in our training data:\n",
    "- For our word pairs (input word with each target word)\n",
    "SGNS creates positive label (1) on its own\n",
    "\n",
    "\n",
    "![](https://www.mapxp.app/BUSI488/sgns-2xs.jpg)\n",
    "\n",
    "\n",
    "**Problem:** We only have positive labels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRxLpedApB2F"
   },
   "source": [
    "\n",
    "### 2.3.2 SGNS Step 2\n",
    "\n",
    "- A classifier such as logistic regression needs both positive and negative samples.\n",
    "- So far, we only have positive samples.\n",
    "\n",
    "**Idea:** Introduce negative samples into our dataset with words that are not neighbors to input word.\n",
    "- Randomly samples words from total vocabulary.\n",
    "\n",
    "![](https://www.mapxp.app/BUSI488/sgns-3.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbKwhGBrpEit"
   },
   "source": [
    "### 2.3.3 SGNS Step 3\n",
    "\n",
    "- For each input-target word pair (i.e., positive samples), draw *N* negative samples (*N* is a hyperparameter)\n",
    "- Calculate similarity of input word to context candidates (i.e., target word and negative samples) using dot product of their vectors\n",
    "- Turn similarity scores into something that looks like probabilities using the sigmoid function (i.e., [logistic operation](https://en.wikipedia.org/wiki/Logistic_function) where you \"squash\" value in range 0 to 1 along an s-shape curve)\n",
    "- Subtract sigmoid from target to get error: `Error = Target - Sigmoid`\n",
    "\n",
    "For a more elaborate explanation, see this great [tutorial](https://jalammar.github.io/illustrated-word2vec/).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mthdBRYiqrbO"
   },
   "source": [
    "### 2.3.3 SGNS Step 4\n",
    "\n",
    "Adjust (or update) vectors in word embedding (i.e., hidden weights matrix) using backpropagation based on the errors of step 3 such that the prediction error is minimized for:\n",
    "\n",
    "  - input word vector\n",
    "  - target word vector\n",
    "  - vectors of negative samples\n",
    "\n",
    "Continue for other word-pairs\n",
    "\n",
    "For a more formal and technical explanation see this great [tutorial](https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45XKkF6z7bYi"
   },
   "source": [
    "# 3. Recommender System for Books based on Text\n",
    "\n",
    "**Problem:** What book would you recommend to a customer whose favorite book is a murder-mystery by Agatha Christie?\n",
    "\n",
    "**Question:** What book features can we use to build a recommender system for books?\n",
    "\n",
    "What about ***Structured Data*** such as features of books?\n",
    "\n",
    "- We can't really use features like the number of pages, is the book hardbound etc. because they aren't very informative of the content of the book.\n",
    "- We could recommend books to her by the same author but then that limits the diversity of recommendations\n",
    "- Genre may be a helpful feature but there are so many books under the category of murder-mystery\n",
    "- If we knew of other users who maybe clicked on some other books online, after reading the same Agatha Christie book, we could use that information for recommendation - but that information can be noisy.\n",
    "\n",
    "What about ***Unstructured Data*** such as content of books?\n",
    "- Requires a way to encode the content of a book summary such that computers can understand it and recommend similar books.\n",
    "\n",
    "**Idea: Recommender System using Word2Vec**   \n",
    "If we can identify similar books based on the word vectors of their descriptions, then we can easily have a machine recommend books to customers.   \n",
    "\n",
    "***Advantage:*** Because we can find similar books based on the words in books, our approach will also work for future books that we have not seen yet!\n",
    "\n",
    "***Challenge:*** We've learned that we can encode words as vectors. But can we do that for an entire book description?   \n",
    "\n",
    "Let's give it a try!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeHRQCgKgodG"
   },
   "source": [
    "- When we discussed operations on vectors we learned that we can ***average vectors*** to obtain a ***single vector to represent the content***.\n",
    "\n",
    "- We can do the same with books:\n",
    "  - Average the word vectors of the words in the book description\n",
    "  - to obtain a single word vector that encodes the book description\n",
    "\n",
    "- In this content-based book recommendation system, we recommend books to a user by considering the similarity in book descriptions.\n",
    "- We find similar books by computing Cosine Similarity of the book embeddings and then recommend those similar books to the user.\n",
    "- Book embeddings are computed using Word embeddings from the Word2Vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A6yK4gcj7hoU"
   },
   "source": [
    "# 1. Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEMdcsDTXBIS"
   },
   "source": [
    "### 3.1 Dataset\n",
    "\n",
    "- We use a dataset that was obtained by scraping book details from [Goodreads](https://goodreads.com) pertaining to business and fiction genres.\n",
    "- The data consist of 2382 books.\n",
    "- There are two genres:\n",
    "  - Business (1185 books)\n",
    "  - Fiction (1197 books)\n",
    "- The available book features include:\n",
    "  - Title\n",
    "  - Short description\n",
    "  - Author\n",
    "  - Rating\n",
    "  - Book image link\n",
    "\n",
    " Dataset Source: https://github.com/sdhilip200/Content-Based-Recommendation---Good-Reads-data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "om_cDOynqy5M"
   },
   "source": [
    "# 1. Read the data (This is in the data subfolder -- load to your Google Drive if necessary)\n",
    "books = pd.read_csv(\"books.csv\", index_col=False)\n",
    "books.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "display(books.Desc[2])"
   ],
   "metadata": {
    "id": "CD64qWgswWz1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr3vkmpjt0w2"
   },
   "source": [
    "## 3.2 Text Preprocessing\n",
    "\n",
    "Since we want to use the book description to compute embeddings, we first clean the book description (i.e. text preprocessing) and store the cleaned description in a new variable called ‘cleaned’."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2DJor5ALt5Mq"
   },
   "source": [
    "# 1. Utitlity functions\n",
    "# a. Remove ASCII\n",
    "def remove_non_ascii(s):\n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "# b. Make all words lower case\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "# c. Remove stop words\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "# d. Remove HTML\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "# e. Remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pl4XubcnIhh3"
   },
   "source": [
    "# Let's clean up!\n",
    "\n",
    "# 2. Make sure the descritions are of type string and create new column to operate on \"cleaned\"\n",
    "books['cleaned'] = books['Desc'].astype(str)\n",
    "\n",
    "# 3. Pass data into utility functions\n",
    "books['cleaned'] = books.cleaned.apply(remove_non_ascii)\n",
    "books['cleaned'] = books.cleaned.apply(make_lower_case)\n",
    "books['cleaned'] = books.cleaned.apply(remove_stop_words)\n",
    "books['cleaned'] = books.cleaned.apply(remove_punctuation)\n",
    "books['cleaned'] = books.cleaned.apply(remove_html)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "books['cleaned']"
   ],
   "metadata": {
    "id": "jeGbjwUYT6br"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6YzatcXO4Ul"
   },
   "source": [
    "We then split each description into word tokens and store it in a list called ‘corpus’ for training our word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OdPBJYrbO_7i"
   },
   "source": [
    "# 4. Split the cleaned description into word tokens\n",
    "corpus = []\n",
    "for row in books['cleaned']:\n",
    "    corpus.append(row.split())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMh8769YJ0_u"
   },
   "source": [
    "## 3.3 Transfer Learning: Fine-Tuning a Word2Vec Model\n",
    "\n",
    "**Problem:**\n",
    "* ***Training*** our own word embeddings is an ***expensive process*** (compute power and time) and also requires a large dataset. We don’t have a large dataset as we scraped Goodreads data which only pertains to the genres of business and fiction.\n",
    "\n",
    "**Solution:**\n",
    "* Use Google ***pre-trained word embeddings***, which were trained on a large corpus, including Wikipedia, news articles and more.\n",
    "\n",
    "**Problem:**\n",
    "* However, since the model was trained on a large corpus including Wikipedia articles, news etc. it ***may not have a similar vocabulary as the Goodreads dataset***. For example: It may not know that the words `Harry` and `Hermione` should be close together and may not even contain the word `Hermione` in its vocabulary, since it may not have come across Harry Potter\n",
    "\n",
    "In order to add the previously unseen words, present in the Goodreads vocabulary to the model, and we fine-tune the pre-trained model on the Goodreads dataset corpus.\n",
    "\n",
    "**Solution:**\n",
    "* We can add unseen words to the vocabulary of the model and teach it new contexts of words by **fine-tuning a pre-trained model**. This means that we use the weights of the existing pre-trained model and adjust them through training, so it learns the new information present in the Goodreads corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-3bYBrpS8gxV"
   },
   "source": [
    "# 0 If you have not loaded the model yet, then run this code:\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300-500k.bin.gz'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hyjbwy3ta6B-"
   },
   "source": [
    "# 1. Initialise the Word2Vec Model with embedding vector_size = 300, window = 5\n",
    "# and min_count = 2 i.e. the minimum number of times a word should appear in the corpus\n",
    "google_model = Word2Vec(vector_size=300, window=5, min_count=2, seed=42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "atyMRgHiPgFx"
   },
   "source": [
    "%%time\n",
    "# Note: We have downloaded the pre-trained model already when visualising word vectors using t-SNE\n",
    "\n",
    "#Step 1: We start by incorporating the vocabulary of our specific corpus into the google model\n",
    "google_model.build_vocab(corpus)\n",
    "\n",
    "#Step 2: We obtain the word vectors for the words in the vocabulary that are already present in pre-trained word2vec model.\n",
    "google_model.wv.vectors_lockf = np.ones(len(google_model.wv))\n",
    "google_model.wv.intersect_word2vec_format(EMBEDDING_FILE, lockf=1.0, binary=True)\n",
    "\n",
    "#Step 3: We fine-tune the model to learn new contexts of words in our corpus\n",
    "google_model.train(corpus, total_examples=google_model.corpus_count, epochs = 5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8krefmYTUXE-"
   },
   "source": [
    "# 2. Print the top 10 most similar words using cosine similarity\n",
    "google_model.wv.most_similar(positive=[\"business\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SRi3fODnUZXR"
   },
   "source": [
    "google_model.wv.most_similar(positive=[\"castle\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT1laFwsBjIs"
   },
   "source": [
    "We now have a Word2Vec model with word vectors fine-tuned on the Goodreads Corpus.\n",
    "\n",
    "However, in our problem, we have book descriptions, which are sequences of words. We need to convert book descriptions into vectors to find the similarity between them and recommend books.\n",
    "\n",
    "To obtain vectors for the book descriptions, we need some way of combining the individual word vectors in the description to obtain a vector for the book.\n",
    "\n",
    "**Idea:** Average the vectors of all words in a book's description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6dSZRdYB0_C"
   },
   "source": [
    "## 3.4 Averaging Word Vectors\n",
    "\n",
    "- As you now know, Word2vec takes a word and gives a D-dimension vector.\n",
    "- Since we have book descriptions which are sentences or sequences of words, we first need to split the sentences into words and find the vectors representation for each word in the sentence.\n",
    "\n",
    "**Consider a book description has N words. Let’s denote the words as** ${w_1, w_2, w_3, w_4 …w_N}$.\n",
    "\n",
    "1. We first calculate Word2Vec for all the N words.\n",
    "2. We then sum all the word vectors and divide the same by the total number of words in the description to obtain the average word vector (V1).\n",
    "\n",
    "$${ V_1 = \\frac{W2V(w_1) + W2V(w_2) + W2V(w_3) + W2V(w_4) + ….. + W2V(w_N)}{N} }$$\n",
    "\n",
    "- Here, vectors are in D-dimensional space, where D = 300.\n",
    "\n",
    "- N = number of words in description 1\n",
    "\n",
    "- v1 = vector representation of book description 1\n",
    "\n",
    "- In this way, each book description can be converted into a unique vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xjMXVeZADYhE"
   },
   "source": [
    "%%time\n",
    "# Generate the average word2vec for the each book description\n",
    "\n",
    "# 1. Define a function that we can call for each book description\n",
    "def average_vectors(df):\n",
    "    # Creating a list for storing the vectors (description into vectors)\n",
    "    embeddings = []\n",
    "\n",
    "    # For each book, create an average vector from all the words in its description\n",
    "    for line in df['cleaned']:\n",
    "        avgword2vec = None\n",
    "        count = 0\n",
    "        for word in line.split():\n",
    "            if word in google_model.wv.index_to_key:\n",
    "                count += 1\n",
    "                if avgword2vec is None:\n",
    "                    avgword2vec = google_model.wv[word]\n",
    "                else:\n",
    "                    avgword2vec = avgword2vec + google_model.wv[word]\n",
    "\n",
    "        if avgword2vec is not None:\n",
    "            avgword2vec = avgword2vec / count\n",
    "            embeddings.append(avgword2vec)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# 2. Apply function to all book descriptions\n",
    "book_embeddings = average_vectors(books)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "book_embeddings[0]"
   ],
   "metadata": {
    "id": "IXfY5x29cS30"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mksw7ndtGBOg"
   },
   "source": [
    "## 3.5 Recommend Books\n",
    "\n",
    "We now use these average book embeddings to find similar books and make recommendations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1J3vtyzXGJk3"
   },
   "source": [
    "# Function to recommend the Top 5 similar books\n",
    "def recommendations(title, book_embeddings, books):\n",
    "\n",
    "    # 1. Find cosine similarity for the vectors\n",
    "    cosine_similarities = cosine_similarity(book_embeddings, book_embeddings)\n",
    "\n",
    "    # 2. Take the title and book image link and store in new data frame called books\n",
    "    book_covers = books[['title', 'image_link']]\n",
    "\n",
    "    # 3. Create a mapping from book title to index\n",
    "    indices = pd.Series(books.index, index = books['title']).drop_duplicates()\n",
    "\n",
    "    # 4. Get the index of the book title we need recommendations from\n",
    "    idx = indices[title]\n",
    "\n",
    "    # 5. Obtain the cosine similarities of the title with all other book from the similarity matrix\n",
    "    # Sim scores is a list of tuples of (book_index, cosine_simialrity)\n",
    "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "\n",
    "    # 6. Sort similarity scores by descending order\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    # 7. Pick the top 5 most similar books. Exclude index 0 as that would be the book itself\n",
    "    sim_scores = sim_scores[1:6]\n",
    "\n",
    "    # 8. Obtain the book indices of the top 5 books\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # 9.Obtain the book cover and title of the top 5 books\n",
    "    recommend = book_covers.iloc[book_indices]\n",
    "    recommend.reset_index(inplace=True)\n",
    "\n",
    "    # 10. Output the book image\n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 15))\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    for index, row in recommend.iterrows():\n",
    "        response = requests.get(row['image_link'])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        axes[index].imshow(img)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fGCf-hUdMbTz"
   },
   "source": [
    "# Let's call our function and pass it a book title to get recommendations for similar books\n",
    "#recommendations(\"Harry Potter and the Prisoner of Azkaban\", book_embeddings, books)\n",
    "recommendations(\"Angels & Demons\", book_embeddings, books)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "801vP4Qyp-sv"
   },
   "source": [
    "\n",
    "We see that the model recommends other Harry Potter books and some other fiction novels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cd-jUqWfVODa"
   },
   "source": [
    "# Let's call our function and pass it a book title to get recommendations for similar books\n",
    "recommendations(\"Steve Jobs\", book_embeddings, books)\n",
    "recommendations(\"The Four Pillars of Investing\", book_embeddings, books)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5E-KN1pDqUc_"
   },
   "source": [
    "We see that the model recommends other non-fiction books related to tech and business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTAzlESWRVNP"
   },
   "source": [
    "# 4. Word2Vec for Assortment Management\n",
    "\n",
    "![Word2Vec for Assortment Management](https://www.mapxp.app/BUSI488/prod2vec.jpg \"Word2Vec for Assortment Management\")\n",
    "\n",
    "\n",
    "Check-out the two studies below that demonstrate how word2vec can be used for basket analysis and assortment management. These studies use models that you have already learned about in this course including k-means,t-SNE and word2vec:\n",
    "\n",
    "- **Item2Vec: Neural Item Embedding for Collaborative Filtering** https://arxiv.org/abs/1603.04259\n",
    "- **P2V-MAP: Mapping Market Structures for Large Retail Assortments** https://journals.sagepub.com/doi/10.1177/0022243719833631"
   ]
  }
 ]
}
